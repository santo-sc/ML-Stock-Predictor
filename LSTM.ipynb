{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40c2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader.data as web\n",
    "from Sequence import sequence\n",
    "from directional_accuracy import calculate_directional_accuracy\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac29036",
   "metadata": {},
   "source": [
    "Data imports from Yahoo Finance and Fedral Reserve Economic Data (fred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea23ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MSFT']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    }
   ],
   "source": [
    "ticker = 'MSFT'\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2020-01-01'\n",
    "sequence_length = 30\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "gdp = web.DataReader('GDP', 'fred', start_date, end_date) #GDP for each quarter\n",
    "cpi = web.DataReader('CPIAUCSL', 'fred', start_date, end_date)\n",
    "unemployment = web.DataReader('UNRATE', 'fred', start_date, end_date)\n",
    "interest_rates = web.DataReader('FEDFUNDS', 'fred', start_date, end_date)\n",
    "\n",
    "close_data = data[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6b507",
   "metadata": {},
   "source": [
    "Data Processing for incorporating Macroeconomic Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4a2ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The dataset is too small for the chosen sequence length. Reduce sequence_length or use more data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#edge case with data:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(combined_data) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is too small for the chosen sequence length. Reduce sequence_length or use more data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(combined_data))\n\u001b[0;32m     25\u001b[0m train_data \u001b[38;5;241m=\u001b[39m combined_data[:train_size]\n",
      "\u001b[1;31mValueError\u001b[0m: The dataset is too small for the chosen sequence length. Reduce sequence_length or use more data."
     ]
    }
   ],
   "source": [
    "def sequence_macro(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i, :]) \n",
    "        y.append(data[i, 0])  \n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "#concat and resample\n",
    "#EDIT HERE TO CHOOSE DIFFERENT MACRO ECONOMIC INPUTS\n",
    "external_data = pd.concat([cpi, unemployment, interest_rates], axis=1)\n",
    "external_data = external_data.resample('D').ffill() #This will give us this data from each day\n",
    "\n",
    "#Merging with stock prices\n",
    "combined_data = pd.concat([close_data, external_data], axis=1).dropna() \n",
    "combined_data.columns = ['Close Price', 'CPI', 'Unemployment', 'Interest Rate']\n",
    "\n",
    "#Must split the data before creating sequences & scaling to prevent leaks\n",
    "min_test_size = sequence_length + 1\n",
    "\n",
    "#edge case with data:\n",
    "if len(combined_data) <= sequence_length + 1:\n",
    "    raise ValueError(\"The dataset is too small for the chosen sequence length. Reduce sequence_length or use more data.\")\n",
    "\n",
    "train_size = int(0.8 * len(combined_data))\n",
    "train_data = combined_data[:train_size]\n",
    "test_data = combined_data[train_size:]\n",
    "\n",
    "\n",
    "#scaling - fit scale to training then apply to test\n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_data = scale.fit_transform(train_data)\n",
    "scaled_test_data = scale.transform(test_data)\n",
    "\n",
    "\n",
    "#create the sequences - using sequence_macro\n",
    "x_train, y_train = sequence_macro(scaled_train_data, sequence_length)\n",
    "x_test, y_test = sequence_macro(scaled_test_data, sequence_length)\n",
    "print(f\"Training data shape: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Testing data shape: X={x_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222f34b",
   "metadata": {},
   "source": [
    "Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(units=1, kernel_regularizer=tf.keras.regularizers.l2(0))\n",
    "])\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.Huber())\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=5,            # Stop after 5 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "model.fit(x_train, y_train, epochs = 50, batch_size=16, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#data shaping with extra data\n",
    "predicted_prices = model.predict(x_test)\n",
    "predicted_prices_full_stack = np.hstack((\n",
    "    predicted_prices,\n",
    "    np.zeros((predicted_prices.shape[0], scaled_train_data.shape[1] - 1))\n",
    "))\n",
    "predicted_prices = scale.inverse_transform(predicted_prices_full_stack)[:, 0]\n",
    "\n",
    "actual_prices_full_stack = np.hstack((\n",
    "\n",
    "    y_test.reshape(-1,1),\n",
    "    np.zeros((y_test.shape[0], scaled_train_data.shape[1] - 1))\n",
    "))\n",
    "actual_prices = scale.inverse_transform(actual_prices_full_stack)[:,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771969a",
   "metadata": {},
   "source": [
    "RMSE and Directional Accuracy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f87615",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))\n",
    "\n",
    "\n",
    "print(f\"Test {ticker} RMSE: {rmse}\")\n",
    "\n",
    "direction_accuracy = calculate_directional_accuracy(actual_prices, predicted_prices)\n",
    "print(f\"Test {ticker} Directional Accuracy: {direction_accuracy}%\")\n",
    "\n",
    "test_dates = data.index[-len(actual_prices):]\n",
    "\n",
    "#print(f\"Actual Prices: {actual_prices[:5]}\")  # Check first 5 values\n",
    "#print(f\"Predicted Prices: {predicted_prices[:5]}\")\n",
    "#print(test_dates)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates, actual_prices, label=\"Actual Prices\", color=\"blue\")\n",
    "plt.plot(test_dates, predicted_prices, label=\"Predicted Prices\", color=\"red\")\n",
    "plt.title(\"Real Prices(blue) Vs. Model Prediction(red)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Macro plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
